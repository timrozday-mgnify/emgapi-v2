---
version: '3'

includes:
  ebi-wp-k8s-hl:
    taskfile: ./deployment/ebi-wp-k8s-hl/Taskfile.yaml
    dir: ./deployment/ebi-wp-k8s-hl

tasks:
  prefect:
    desc: "Run a command on the prefect CLI. E.g. task prefect -- deployment"
    cmds:
      - docker compose run app python manage.py prefectcli {{.CLI_ARGS}}

  create-pool:
    desc: "Create a Prefect workpool. E.g. NAME=slurm task create-process-pool"
    cmds:
      - docker compose exec -e PREFECT_CLIENT_RETRY_EXTRA_CODES=0 app python manage.py prefectcli work-pool create --type {{.TYPE}} {{.CLI_ARGS}} {{.NAME}}
    vars:
      TYPE: "process"
      NAME: "slurm"

  deploy-flow:
    desc: "Build and deploy a prefect flow. E.g. FLOW=mgnify_run_pipeline_flow task deploy-flow"
    cmds:
      - docker compose --profile prefect up -d
      - docker compose run -e PREFECT_CLIENT_RETRY_EXTRA_CODES=0 app python manage.py prefectcli deploy {{ .FILE | default (list "workflows/flows/" .FLOW ".py" | join "") }}:{{.FLOW}} --name {{.FLOW}}_deployment -p {{.POOL}} --prefect-file workflows/prefect_deployments/prefect-dev-donco.yaml
    vars:
      POOL: "slurm"
    requires:
      vars: [FLOW]

  run:
    desc: "Run everything"
    cmds:
      - docker compose --profile all up {{.CLI_ARGS}}

  run-api-only:
    desc: "Run just the API (no slurm)"
    cmds:
      - docker compose --profile app up {{.CLI_ARGS}}

  stop:
    desc: "Stop everything"
    cmds:
      - docker compose --profile all down

  manage:
    desc: "Run a management command. E.g. task manage -- migrate"
    cmds:
      - docker compose run app python manage.py {{.CLI_ARGS}}

  sbatch:
    desc: "Dispatch a slurm job to the mini slurm cluster. E.g. task sbatch -- --wait -t 0:00:30 --mem=10M --wrap=\"echo 'hello world'\" -o hello-world.out"
    cmds:
      - docker exec slurm_node "sbatch {{.CLI_ARGS}}"

  slurm:
    desc: "Connect to the slurm node to run commands there."
    cmds:
      - docker exec -it slurm_node bash
    interactive: true

  agent:
    desc: "Connect to the prefect agent to run commands there."
    cmds:
      - docker exec -it prefect-agent bash
    interactive: true

  test:
    desc: "Run pytest tests. E.g. `task test` or `task test -- -k study`"
    cmds:
      - docker compose run --entrypoint /bin/bash app -c "pytest {{.CLI_ARGS}}"

  testk:
    desc: "Run pytest for a subset of unit tests, without coverage noise. E.g. `task testk -- amplicon`"
    cmds:
      - docker compose run --entrypoint /bin/bash app -c "pytest --no-cov -s -p no:sugar -k {{.CLI_ARGS}}"

  make-dev-data:
    desc: "Populate the app database with some demo data for development purposes"
    prompt: "This asks for a password in a moment, but this IS NOT your computer password. It is a new one (anything you like) just to log into the demo admin panel as emgdev. Okay?"
    cmds:
      - docker compose --profile all down
      - docker volume rm -f emgapiv2_appdb  # force so that no error if volume doesn't exist.
      - docker compose run --entrypoint /bin/bash app -c "pytest -m dev_data_maker --no-cov -s"  # dumps a json to dev-db.json
      - docker compose run app python manage.py migrate
      - docker compose run app python manage.py loaddata dev-db.json
      - docker compose run app python manage.py createsuperuser --username emgdev --email emg@example.org
      - rm -fr slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398
      - mkdir -p slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/
      - cp -r slurm-dev-environment/fs/nfs/public/tests/amplicon_v6_output/SRR1111111 slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/amplicon
      - mkdir -p slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/study-summaries
      - cp slurm-dev-environment/fs/nfs/public/tests/amplicon_v6_output/*study_summary.tsv slurm-dev-environment/fs/nfs/ftp/public/databases/metagenomics/mgnify_results/PRJNA398/PRJNA398089/study-summaries/
      - docker compose run app python manage.py shell -c "a=Analysis.objects.get(accession='MGYA00000004'); a.external_results_dir='PRJNA398/PRJNA398089/SRR1111/SRR1111111/V6/amplicon'; a.save()"

  debug-break-prefect-connection:
    desc: "Temporarily return 404s from the prefect server, when prefect worker tries to access it"
    vars:
      DURATION: 60
    cmds:
      - docker exec -d prefect-agent touch /tmp/prefect_is_404
      - echo "Interceptor active! Prefect API calls will now return 404."
      - sleep {{.DURATION}}
      - docker exec prefect-agent rm /tmp/prefect_is_404
    silent: false
    summary: |
      To test connection interruption, e.g.:
      $  FLOW=slow_cluster_job_example task deploy-flow
      $  task prefect -- deployment run  'Slow running cluster job example/slow_cluster_job_example_deployment' -p run_for_seconds=120
      ... wait for that flow to start its cluster job in prefect UI ...
      $ task debug-break-prefect-connection
      ... you will see that the worker stops reporting logs for a while, but should not crash thanks to retries ...

  debug-make-prefect-flow-with-zombie-job:
    desc: "Make a prefect flow in which there is a zombie cluster job (one where prefect worker died whilst running slurm job)"
    cmds:
      - task: prefect
        vars:
          {CLI_ARGS: "deployment run 'Slow running cluster job example/slow_cluster_job_example_deployment' -p run_for_seconds=60 -p slurm_resubmit_policy_minutes_ago=5"}
      - sleep 30
      - docker compose kill prefect-agent
      - sleep 60
      - docker compose start prefect-agent
    summary: |
      To make jobs in zombie state, for developing cleanup tools and investigating state management etc.
      You need to have deployed the `slow_cluster_job_example` flow to use this one, e.g.
      $ FLOW=slow_cluster_job_example task deploy-flow
      The task sets off a flow, that runs a 60second cluster job.
      ~Half way through that job, the prefect worker is killed for 60 seconds,
      so the cluster job should have finished whilst the worker was offline.
      This results in a zombie state flow and subflow, because the prefect server assumes the worker will
      somehow pick up where it left off.

      Can then use e.g.
      $ task manage -- reconnect_zombie_jobs -t 30
      to try and cleanup/retry the zombie flow.

      Since slurm_resubmit_policy_minutes_ago=5,
      if you run the reconnect_zombie_jobs shortly after the zombie job is made,
      you can verify that the slurm job actually continued to run or completed in the background and is
      "reconnected" to by the restarted flow.
      This is desired behaviour.

  generate-ena-util-stub:
    desc: "(Re)Generate an ena utils request stub file for a given result type of the ENA Portal API. Usage: task generate-ena-util-study -- study"
    cmds:
      - docker compose run --entrypoint /bin/bash app -c "python workflows/ena_utils/generate_ena_model.py {{.CLI_ARGS}}" >> workflows/ena_utils/{{.CLI_ARGS}}.py
      - echo "See ./workflows/ena_utils/{{.CLI_ARGS}}.py"
      - echo "You may also need to update workflows/ena_utils/requestors.py:ENAAPIRequest"
